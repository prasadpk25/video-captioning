{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdbd746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_training.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sentence_transformers import SentenceTransformer  # CLIP wrapper\n",
    "from glob import glob\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b386e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -U sentence-transformers\n",
    "! pip install -U transformers==4.44.2\n",
    "! pip install -U sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152d4fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725b7860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Dataset wrapper (expects pre-extracted frames per video and a label file)\n",
    "class TVSumFramesDataset(Dataset):\n",
    "    def __init__(self, list_of_videos, embedding_model):\n",
    "        # list_of_videos: list of dicts: {\"frames\": [paths...], \"scores\": [float...]}\n",
    "        self.data = list_of_videos\n",
    "        self.embedder = embedding_model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        frames = entry[\"frames\"]  # ordered list of image paths\n",
    "        scores = np.array(entry[\"scores\"], dtype=np.float32)  # same length\n",
    "        # extract embeddings for frames (batch encode)\n",
    "        imgs = [Image.open(p).convert('RGB') for p in frames]\n",
    "        embs = self.embedder.encode(imgs, convert_to_tensor=True).cpu().numpy()  # (T, D)\n",
    "        return torch.tensor(embs, dtype=torch.float32), torch.tensor(scores, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d8771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Temporal regressor (BiLSTM + MLP)\n",
    "class TemporalRegressor(nn.Module):\n",
    "    def __init__(self, input_dim=512, hidden=256, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden*2, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 1),\n",
    "            nn.Sigmoid()  # output normalized 0..1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: (B, T, D)\n",
    "        out, _ = self.lstm(x)  # (B, T, 2*hidden)\n",
    "        scores = self.fc(out).squeeze(-1)  # (B, T)\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3200cc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Simple training loop\n",
    "def collate_fn(batch):\n",
    "    # Pads sequences to max length in batch\n",
    "    embs, scores = zip(*batch)\n",
    "    lengths = [e.shape[0] for e in embs]\n",
    "    maxlen = max(lengths)\n",
    "    D = embs[0].shape[1]\n",
    "    emb_pad = torch.zeros(len(embs), maxlen, D, dtype=torch.float32)\n",
    "    score_pad = torch.zeros(len(embs), maxlen, dtype=torch.float32)\n",
    "    mask = torch.zeros(len(embs), maxlen, dtype=torch.bool)\n",
    "    for i,(e,s) in enumerate(zip(embs,scores)):\n",
    "        L = e.shape[0]\n",
    "        emb_pad[i,:L] = e\n",
    "        score_pad[i,:L] = s\n",
    "        mask[i,:L] = 1\n",
    "    return emb_pad.to(DEVICE), score_pad.to(DEVICE), mask.to(DEVICE)\n",
    "\n",
    "\n",
    "def train(dataset, epochs=10, batch_size=4, lr=1e-4):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    model = TemporalRegressor(input_dim=dataset[0][0].shape[1]).to(DEVICE)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss(reduction='none')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for embs, scores, mask in loader:\n",
    "            pred = model(embs)  # (B,T)\n",
    "            loss_map = loss_fn(pred, scores) * mask.float()\n",
    "            loss = loss_map.sum() / mask.float().sum()\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs} loss: {epoch_loss/len(loader):.4f}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b90efa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) Inference: get scores, smooth, pick peaks\n",
    "import scipy.signal as signal\n",
    "def select_keyframes(embs, model, top_k=5, smooth_win=5):\n",
    "    # embs: numpy (T, D) or tensor\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor(embs, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "        scores = model(x).cpu().numpy().squeeze(0)  # (T,)\n",
    "    # smooth\n",
    "    scores_s = np.convolve(scores, np.ones(smooth_win)/smooth_win, mode='same')\n",
    "    # find peaks (local maxima)\n",
    "    peaks, _ = signal.find_peaks(scores_s, distance= max(1,int(len(scores_s)/ (top_k*2))))\n",
    "    # if not enough peaks, take top-k by score\n",
    "    if len(peaks) < top_k:\n",
    "        idxs = np.argsort(scores_s)[-top_k:]\n",
    "    else:\n",
    "        idxs = peaks[np.argsort(scores_s[peaks])][-top_k:]\n",
    "    idxs = np.sort(idxs)\n",
    "    return idxs, scores_s\n",
    "\n",
    "# -------------------------\n",
    "# Example usage (high level):\n",
    "# 1) Build `list_of_videos` for TVSum: each entry has frame paths and TVSum human scores aligned to sampled frames.\n",
    "# 2) embedder = SentenceTransformer('clip-ViT-B-32')\n",
    "# 3) dataset = TVSumFramesDataset(list_of_videos, embedder)\n",
    "# 4) model = train(dataset)\n",
    "# 5) For new video: sample frames -> embed with embedder -> select_keyframes(embs, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b446af0",
   "metadata": {},
   "source": [
    "new pipeline working \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243eac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just try build in and atlest generate some thing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a5d83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames: 5638\n",
      "‚úÖ Extracted 5638 frames to frames_all/\n"
     ]
    }
   ],
   "source": [
    "import cv2, os\n",
    "\n",
    "video_path = \"Lion vs. Wildebeest_ How Lions Hunt as a Pride (1).mp4\"\n",
    "output_dir = \"frames_all\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(f\"Total frames: {frame_count}\")\n",
    "\n",
    "count = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    path = os.path.join(output_dir, f\"frame_{count:05d}.jpg\")\n",
    "    cv2.imwrite(path, frame)\n",
    "    count += 1\n",
    "\n",
    "cap.release()\n",
    "print(f\"‚úÖ Extracted {count} frames to {output_dir}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefd7a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\open_clip\\factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Frame embedded successfully! Shape: (512,)\n"
     ]
    }
   ],
   "source": [
    "import torch, open_clip, os\n",
    "from PIL import Image\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "img_path = \"frames/frame_001.jpg\"  # now this exists\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "img_tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    emb = model.encode_image(img_tensor)\n",
    "    emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "    emb = emb.cpu().numpy().squeeze(0)\n",
    "\n",
    "print(\"‚úÖ Frame embedded successfully! Shape:\", emb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7941f794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding frames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5638/5638 [09:55<00:00,  9.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All frame embeddings ready: (5638, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch, open_clip, os\n",
    "from PIL import Image\n",
    "\n",
    "frames_dir = \"frames_all\"\n",
    "frame_files = sorted([os.path.join(frames_dir, f) for f in os.listdir(frames_dir) if f.endswith(\".jpg\")])\n",
    "\n",
    "embeddings = []\n",
    "for f in tqdm(frame_files, desc=\"Embedding frames\"):\n",
    "    img = Image.open(f).convert(\"RGB\")\n",
    "    img_tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = model.encode_image(img_tensor)\n",
    "        emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "    embeddings.append(emb.cpu().numpy().squeeze(0))\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "print(\"‚úÖ All frame embeddings ready:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0d159f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Selected 86 keyframes out of 5638\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "threshold = 0.85   # try 0.8 if too few frames remain, 0.9 if too many\n",
    "keyframes = [0]    # keep the first frame by default\n",
    "\n",
    "for i in range(1, len(embeddings)):\n",
    "    sim = cosine_similarity([embeddings[i]], [embeddings[keyframes[-1]]])[0][0]\n",
    "    if sim < threshold:\n",
    "        keyframes.append(i)\n",
    "\n",
    "print(f\"‚úÖ Selected {len(keyframes)} keyframes out of {len(embeddings)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c25bcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 86 keyframes to keyframes2/\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"keyframes2\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for idx in keyframes:\n",
    "    img_path = frame_files[idx]\n",
    "    img = Image.open(img_path)\n",
    "    img.save(os.path.join(output_dir, os.path.basename(img_path)))\n",
    "\n",
    "print(f\"‚úÖ Saved {len(keyframes)} keyframes to {output_dir}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c48cce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install transformers==4.30.2 timm==0.6.13 pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d158ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "üé• Video\n",
    "‚Üì\n",
    "üß© Extract frames (scene-change based)\n",
    "‚Üì\n",
    "üñºÔ∏è Caption frames (BLIP)\n",
    "‚Üì\n",
    "üî¢ Compute embeddings ‚Üí weights\n",
    "‚Üì\n",
    "üéûÔ∏è Cluster similar frames (scenes)\n",
    "‚Üì\n",
    "‚úçÔ∏è Weighted + diverse caption aggregation\n",
    "‚Üì\n",
    "üß† Summarization (BART / T5 / LLaMA)\n",
    "‚Üì\n",
    "üìú Final summary (context-rich & highlight-aware)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043ca794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Load BLIP model\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Folder containing keyframes\n",
    "frames_folder = \"keyframes\"   # path where your 43 keyframes are saved\n",
    "output_captions = {}\n",
    "\n",
    "# Generate captions\n",
    "for frame in sorted(os.listdir(frames_folder)):\n",
    "    if frame.endswith((\".jpg\", \".png\")):\n",
    "        img_path = os.path.join(frames_folder, frame)\n",
    "        raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        inputs = processor(raw_image, return_tensors=\"pt\").to(model.device)\n",
    "        out = model.generate(**inputs, max_new_tokens=30)\n",
    "\n",
    "        caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "        output_captions[frame] = caption\n",
    "        print(f\"{frame}: {caption}\")\n",
    "\n",
    "# Optional: Save to file\n",
    "import json\n",
    "with open(\"captions.json\", \"w\") as f:\n",
    "    json.dump(output_captions, f, indent=4)\n",
    "\n",
    "print(\"‚úÖ Captions generated and saved to captions.json!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf78219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßæ Video Summary:\n",
      "a chephaus chephaus chephaus in the grass two wildes are walking through the tall grass a wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde a lion is walking through the tall grass a cow is standing in a field with a bird a wilde cow is seen in this und - news video a zebra is walking through the tall grass a zebra standing in a field of grass a bird is sitting on a branch in the middle of a field a wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde a bird is standing in the grass a lion and a lioness in the wild a group of buffalos running through a field a bird is flying in the air over a field lion attacks a lion in the wild a wilde and a wilde running in the wild a lion chasing a wilde in the wild lion attacks a lion in the wild a large elephant is running through the grass a wilde running through the grass in the wild a large animal standing in a field a herd of wildes in the wild a bird is standing in the middle of a field a zebra running through the grass in the wild a zebra running through the brush in the wild a wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde a large ant ant ant ant ant ant ant ant ant ant ant ant ant ant ant ant ant ant ant a couple of animals fighting in a field a large bird is standing in the grass a man riding a horse through a field a lion is running through the grass a close up of a horse ' s face a herd of cattle grazing in a dry field a lion is seen in this und - toned video a group of lions walking through a field a lion walking through tall grass in the wild lion cubs play with a dead zebra a large herd of cattle a field with a tree in the middle\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load captions\n",
    "with open(\"captions.json\") as f:\n",
    "    captions = json.load(f)\n",
    "\n",
    "# Step 1: Create embeddings\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "caption_texts = list(captions.values())\n",
    "embeddings = model.encode(caption_texts, convert_to_tensor=True)\n",
    "\n",
    "# Step 2: Compute pairwise similarity matrix\n",
    "cosine_sim = util.cos_sim(embeddings, embeddings).cpu().numpy()\n",
    "\n",
    "# Step 3: Compute weights (e.g., uniqueness)\n",
    "# Lower average similarity = more unique frame = higher importance\n",
    "weights = 1 - cosine_sim.mean(axis=1)\n",
    "\n",
    "# Step 4: Sort frames by weight\n",
    "frame_names = list(captions.keys())\n",
    "ranked = sorted(zip(frame_names, weights), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Top keyframes\n",
    "top_frames = [f for f, w in ranked[:10]]\n",
    "print(\"üéØ Top keyframes:\", top_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cce1253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping transformers as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall -y transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6f59e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a chephaus chephaus chephaus in the grass two wildes are walking through the tall grass a wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde a lion is walking through the tall grass a cow is standing in a field with a bird a wilde cow is seen in this und - news video a zebra is walking through the tall grass a zebra standing in a field of grass a bird is sitting on a branch in the middle of a field a wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde a bird is standing in the grass a lion and a lioness in the wild a group of buffalos running through a field a bird is flying in the air over a field lion attacks a lion in the wild a wilde and a wilde running in the wild a lion chasing a wilde in the wild lion attacks a lion in the wild a large elephant is running through the grass a wilde running through the grass in the wild a large animal standing in a field a herd of wildes in the wild a bird is standing in the middle of a field a zebra running through the grass in the wild a zebra running through the brush in the wild a wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde wilde a large ant ant ant ant ant ant ant ant ant ant ant ant ant ant ant ant ant ant ant a couple of animals fighting in a field a large bird is standing in the grass a man riding a horse through a field a lion is running through the grass a close up of a horse ' s face a herd of cattle grazing in a dry field a lion is seen in this und - toned video a group of lions walking through a field a lion walking through tall grass in the wild lion cubs play with a dead zebra a large herd of cattle a field with a tree in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c86274e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cecb8e",
   "metadata": {},
   "source": [
    "step 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e5bb927",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'clip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18848\\3683000459.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmoviepy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meditor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcatenate_videoclips\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'clip'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import clip\n",
    "import cv2, numpy as np\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "import whisper\n",
    "\n",
    "# =============================\n",
    "# 1. Load pretrained models\n",
    "# =============================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "whisper_model = whisper.load_model(\"large-v3\")\n",
    "\n",
    "# Lightweight temporal attention network\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=heads, batch_first=True)\n",
    "        self.fc = nn.Linear(dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [T, D]\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        scores = self.fc(attn_out).squeeze(-1)  # [T]\n",
    "        weights = torch.softmax(scores, dim=0)\n",
    "        return weights.detach().cpu().numpy()\n",
    "\n",
    "temporal_model = TemporalAttention(dim=512).to(device).eval()\n",
    "\n",
    "# =============================\n",
    "# 2. Frame extraction\n",
    "# =============================\n",
    "def extract_frames(video_path, stride=10):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames, times = [], []\n",
    "    idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        if idx % stride == 0:\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            times.append(cap.get(cv2.CAP_PROP_POS_MSEC))\n",
    "        idx += 1\n",
    "    cap.release()\n",
    "    return frames, times\n",
    "\n",
    "# =============================\n",
    "# 3. CLIP embeddings\n",
    "# =============================\n",
    "def get_clip_embeddings(frames):\n",
    "    embs = []\n",
    "    with torch.no_grad():\n",
    "        for f in frames:\n",
    "            img = clip_preprocess(Image.fromarray(f)).unsqueeze(0).to(device)\n",
    "            feat = clip_model.encode_image(img)\n",
    "            feat = F.normalize(feat, dim=-1)\n",
    "            embs.append(feat)\n",
    "    embs = torch.cat(embs, dim=0)  # [T,512]\n",
    "    return embs\n",
    "\n",
    "# =============================\n",
    "# 4. Compute attention weights\n",
    "# =============================\n",
    "def compute_importance(embs):\n",
    "    with torch.no_grad():\n",
    "        weights = temporal_model(embs.unsqueeze(0)).flatten()\n",
    "    return weights / weights.max()\n",
    "\n",
    "# =============================\n",
    "# 5. Select key frames\n",
    "# =============================\n",
    "def select_keyframes(frames, weights, top_k=10):\n",
    "    idx = np.argsort(weights)[-top_k:]\n",
    "    return [frames[i] for i in idx], weights[idx]\n",
    "\n",
    "# =============================\n",
    "# 6. Whisper transcription\n",
    "# =============================\n",
    "def transcribe_audio(video_path):\n",
    "    result = whisper_model.transcribe(video_path)\n",
    "    return result[\"text\"]\n",
    "\n",
    "# =============================\n",
    "# 7. Main pipeline\n",
    "# =============================\n",
    "from PIL import Image\n",
    "\n",
    "video_path = \"Lion vs. Wildebeest_ How Lions Hunt as a Pride (1).mp4\"\n",
    "frames, times = extract_frames(video_path, stride=15)\n",
    "clip_embs = get_clip_embeddings(frames)\n",
    "importance_weights = compute_importance(clip_embs)\n",
    "key_frames, key_weights = select_keyframes(frames, importance_weights, top_k=12)\n",
    "transcript = transcribe_audio(video_path)\n",
    "\n",
    "print(\"Transcript snippet:\\n\", transcript[:250], \"...\")\n",
    "print(\"\\nKey-frame importance weights:\\n\", key_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d22c2db8",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset 'facebook/hd-vila' doesn't exist on the Hub or cannot be accessed.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDatasetNotFoundError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Try loading a real available video-caption dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfacebook/hd-vila\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalidation[:5]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müé• Video Path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset[i][\u001b[33m'\u001b[39m\u001b[33mvideo\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\datasets\\load.py:1397\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1392\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   1393\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   1394\u001b[39m )\n\u001b[32m   1396\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1397\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1410\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1412\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   1413\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\datasets\\load.py:1137\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     features = _fix_for_backward_compatible_features(features)\n\u001b[32m-> \u001b[39m\u001b[32m1137\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[32m   1147\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\datasets\\load.py:1030\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m   1028\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt reach the Hugging Face Hub for dataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1029\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (DataFilesNotFoundError, DatasetNotFoundError, EmptyDatasetError)):\n\u001b[32m-> \u001b[39m\u001b[32m1030\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1031\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[32m   1032\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1033\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1034\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1035\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\datasets\\load.py:985\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\n\u001b[32m    982\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRevision \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt exist for dataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hub.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    983\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m985\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt exist on the Hub or cannot be accessed.\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    986\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    987\u001b[39m     api.hf_hub_download(\n\u001b[32m    988\u001b[39m         repo_id=path,\n\u001b[32m    989\u001b[39m         filename=filename,\n\u001b[32m   (...)\u001b[39m\u001b[32m    992\u001b[39m         proxies=download_config.proxies,\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[31mDatasetNotFoundError\u001b[39m: Dataset 'facebook/hd-vila' doesn't exist on the Hub or cannot be accessed."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Try loading a real available video-caption dataset\n",
    "dataset = load_dataset(\"facebook/hd-vila\", split=\"validation[:5]\")\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"üé• Video Path: {dataset[i]['video']}\")\n",
    "    print(f\"üìù Caption: {dataset[i]['caption']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627a2ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b2009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
