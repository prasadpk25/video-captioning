{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae1fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ClipInsight Pipeline\n",
    "Input: video file path\n",
    "Output: keyframes, transcript, multimodal fused features, and a text summary\n",
    "\n",
    "Prereqs (install these in your environment):\n",
    "    pip install open-clip-torch==2.0.0  # or `open-clip-torch`\n",
    "    pip install transformers\n",
    "    pip install accelerate\n",
    "    pip install sentence-transformers\n",
    "    pip install git+https://github.com/openai/whisper.git   # or 'whisper' package\n",
    "    pip install torchaudio\n",
    "    pip install moviepy\n",
    "    pip install scikit-learn\n",
    "    pip install tqdm\n",
    "    pip install opencv-python\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import tempfile\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# optional model libs\n",
    "try:\n",
    "    import open_clip\n",
    "except Exception as e:\n",
    "    open_clip = None\n",
    "try:\n",
    "    import whisper\n",
    "except Exception as e:\n",
    "    whisper = None\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast, pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG - change as needed\n",
    "# ---------------------------\n",
    "CONFIG = {\n",
    "    \"VIDEO_PATH\": \"input.mp4\",\n",
    "    \"WORKDIR\": \"clipinsight_output\",\n",
    "    \"FRAME_EXTRACT_FPS\": 3,       # sample fps to extract (change to 30 for all frames)\n",
    "    \"CLIP_MODEL\": (\"ViT-B-32\", \"openai\"),   # open-clip model name + pretrained source\n",
    "    \"USE_WHISPER\": True,\n",
    "    \"WHISPER_MODEL\": \"base\",      # tiny, base, small, medium, large\n",
    "    \"LITE_MODE\": True,            # if True uses smaller models for speed (recommended)\n",
    "    \"NUM_SCENES\": 8,              # number of clusters/scenes for summarization\n",
    "    \"SUMMARY_MODEL\": \"t5-small\",  # or \"t5-base\" / \"facebook/bart-large-cnn\"\n",
    "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"BATCH_SIZE\": 32,             # embedding batch size\n",
    "    \"CACHE\": True,                # cache embeddings / captions to disk\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG[\"WORKDIR\"], exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Utilities\n",
    "# ---------------------------\n",
    "def ffmpeg_extract_audio(video_path: str, out_audio: str):\n",
    "    # uses opencv/ffmpeg via moviepy for cross-platform simplicity\n",
    "    from moviepy.editor import VideoFileClip\n",
    "    clip = VideoFileClip(video_path)\n",
    "    clip.audio.write_audiofile(out_audio, verbose=False, logger=None)\n",
    "    clip.close()\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Frame extraction\n",
    "#    extracts images at sample_fps (not necessarily original fps)\n",
    "# ---------------------------\n",
    "def extract_frames(video_path: str, out_dir: str, sample_fps: float = 3.0) -> List[str]:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    orig_fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "    interval = max(1, int(round(orig_fps / sample_fps)))\n",
    "    saved_paths = []\n",
    "    idx = 0\n",
    "    saved = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if idx % interval == 0:\n",
    "            filename = f\"frame_{saved:06d}.jpg\"\n",
    "            path = os.path.join(out_dir, filename)\n",
    "            cv2.imwrite(path, frame)\n",
    "            saved_paths.append(path)\n",
    "            saved += 1\n",
    "        idx += 1\n",
    "    cap.release()\n",
    "    return saved_paths\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Speech-to-text (Whisper wrapper)\n",
    "# ---------------------------\n",
    "def transcribe_audio_with_whisper(video_path: str, out_txt: str, model_name: str = \"base\"):\n",
    "    if whisper is None:\n",
    "        raise RuntimeError(\"whisper is not installed. pip install git+https://github.com/openai/whisper.git\")\n",
    "    audio_tmp = os.path.join(tempfile.gettempdir(), \"clipinsight_audio.wav\")\n",
    "    ffmpeg_extract_audio(video_path, audio_tmp)\n",
    "    model = whisper.load_model(model_name, device=CONFIG[\"DEVICE\"])\n",
    "    result = model.transcribe(audio_tmp, fp16=(CONFIG[\"DEVICE\"]==\"cuda\"))\n",
    "    transcript = result.get(\"text\",\"\")\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(transcript)\n",
    "    return transcript\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Frame embedding using CLIP (open-clip)\n",
    "# ---------------------------\n",
    "def load_openclip(model_name: str = \"ViT-B-32\", pretrained: str = \"openai\"):\n",
    "    if open_clip is None:\n",
    "        raise RuntimeError(\"open_clip not installed. pip install open-clip-torch\")\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained)\n",
    "    model.eval()\n",
    "    return model.to(CONFIG[\"DEVICE\"]), preprocess\n",
    "\n",
    "def batch_embed_images_openclip(model, preprocess, image_paths: List[str], batch_size: int = 32):\n",
    "    all_embs = []\n",
    "    from torchvision import transforms\n",
    "    import torch\n",
    "    imgs = []\n",
    "    for p in image_paths:\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        imgs.append(preprocess(img))\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(imgs), batch_size):\n",
    "            batch = torch.stack(imgs[i:i+batch_size]).to(CONFIG[\"DEVICE\"])\n",
    "            emb = model.encode_image(batch)\n",
    "            emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "            all_embs.append(emb.cpu().numpy())\n",
    "    if len(all_embs) == 0:\n",
    "        return np.zeros((0,512), dtype=np.float32)\n",
    "    return np.vstack(all_embs)\n",
    "\n",
    "# Alternative: SentenceTransformer image model (fallback)\n",
    "def batch_embed_images_sbert(model_name: str, image_paths: List[str], batch_size: int=32):\n",
    "    s = SentenceTransformer(model_name, device=CONFIG[\"DEVICE\"])\n",
    "    return s.encode(image_paths, batch_size=batch_size, convert_to_tensor=False, show_progress_bar=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Caption generation (BLIP)\n",
    "# ---------------------------\n",
    "def generate_blip_captions(frames: List[str], cache_path: Optional[str] = None) -> Dict[str, str]:\n",
    "    from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "    device = CONFIG[\"DEVICE\"]\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "    captions = {}\n",
    "    for p in tqdm(frames, desc=\"Captioning frames\"):\n",
    "        image = Image.open(p).convert(\"RGB\")\n",
    "        inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "        out = model.generate(**inputs, max_new_tokens=30)\n",
    "        cap = processor.decode(out[0], skip_special_tokens=True)\n",
    "        captions[os.path.basename(p)] = cap\n",
    "    if cache_path:\n",
    "        with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(captions, f, indent=2, ensure_ascii=False)\n",
    "    return captions\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Temporal model (lightweight BiLSTM baseline)\n",
    "#    This is optional for supervised scoring; here we implement a small BiLSTM + MLP\n",
    "# ---------------------------\n",
    "class TemporalScorer(nn.Module):\n",
    "    def __init__(self, input_dim: int = 512, hidden: int = 256, n_layers:int=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden, num_layers=n_layers, batch_first=True, bidirectional=True)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden*2, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, D)\n",
    "        out, _ = self.lstm(x)\n",
    "        scores = self.mlp(out).squeeze(-1)  # (B, T)\n",
    "        return scores\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Multimodal fusion (simple cross-attention module)\n",
    "#    We'll implement a simple fusion: project image emb and text emb to same dim,\n",
    "#    then run a small Transformer encoder to mix them.\n",
    "# ---------------------------\n",
    "class FusionModule(nn.Module):\n",
    "    def __init__(self, img_dim=512, txt_dim=384, hidden=512, num_layers=2, nhead=8):\n",
    "        super().__init__()\n",
    "        self.img_proj = nn.Linear(img_dim, hidden)\n",
    "        self.txt_proj = nn.Linear(txt_dim, hidden)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "    def forward(self, img_embs: torch.Tensor, txt_embs: torch.Tensor):\n",
    "        # img_embs: (T_img, img_dim), txt_embs: (T_txt, txt_dim)\n",
    "        device = img_embs.device\n",
    "        img = self.img_proj(img_embs)        # (T_img, hidden)\n",
    "        txt = self.txt_proj(txt_embs)        # (T_txt, hidden)\n",
    "        concat = torch.cat([img, txt], dim=0).unsqueeze(0)  # (1, T, hidden)\n",
    "        fused = self.transformer(concat)  # (1, T, hidden)\n",
    "        pooled = fused.mean(dim=1)  # (1, hidden)\n",
    "        return pooled.squeeze(0)    # (hidden,)\n",
    "\n",
    "# ---------------------------\n",
    "# 7) Summary generation (T5)\n",
    "# ---------------------------\n",
    "def generate_summary_from_texts(texts: List[str], model_name: str = \"t5-small\", max_len=120):\n",
    "    # combine texts intelligently (you can weight and cluster before calling this)\n",
    "    joined = \" \".join(texts)\n",
    "    tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name).to(CONFIG[\"DEVICE\"])\n",
    "    inputs = tokenizer(\"summarize: \" + joined, return_tensors=\"pt\", truncation=True, max_length=1024).to(CONFIG[\"DEVICE\"])\n",
    "    out = model.generate(**inputs, max_length=max_len, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# ---------------------------\n",
    "# 8) High-level pipeline orchestration\n",
    "# ---------------------------\n",
    "def pipeline_run(video_path: str, workdir: str):\n",
    "    os.makedirs(workdir, exist_ok=True)\n",
    "    frames_dir = os.path.join(workdir, \"frames\")\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "    # 1) extract frames (sampled)\n",
    "    print(\"1) extracting frames ...\")\n",
    "    frames = extract_frames(video_path, frames_dir, sample_fps=CONFIG[\"FRAME_EXTRACT_FPS\"])\n",
    "    print(f\"   frames saved: {len(frames)}\")\n",
    "\n",
    "    # 2) transcribe audio\n",
    "    transcript_path = os.path.join(workdir, \"transcript.txt\")\n",
    "    transcript = \"\"\n",
    "    if CONFIG[\"USE_WHISPER\"]:\n",
    "        print(\"2) transcribing audio with Whisper ...\")\n",
    "        try:\n",
    "            transcript = transcribe_audio_with_whisper(video_path, transcript_path, model_name=CONFIG[\"WHISPER_MODEL\"])\n",
    "        except Exception as e:\n",
    "            print(\"Whisper failed:\", e)\n",
    "    else:\n",
    "        print(\"2) skipping transcript\")\n",
    "\n",
    "    # 3) compute image embeddings (CLIP)\n",
    "    print(\"3) computing CLIP embeddings ...\")\n",
    "    clip_model_choice = CONFIG[\"CLIP_MODEL\"]\n",
    "    if open_clip is None:\n",
    "        raise RuntimeError(\"open_clip not installed. Install open-clip-torch\")\n",
    "    clip_model, preprocess = load_openclip(model_name=clip_model_choice[0], pretrained=clip_model_choice[1])\n",
    "    img_embeddings = batch_embed_images_openclip(clip_model, preprocess, frames, batch_size=CONFIG[\"BATCH_SIZE\"])\n",
    "    # optionally cache\n",
    "    emb_path = os.path.join(workdir, \"img_embeddings.npy\")\n",
    "    if CONFIG[\"CACHE\"]:\n",
    "        np.save(emb_path, img_embeddings)\n",
    "\n",
    "    # 4) caption key frames (BLIP)\n",
    "    print(\"4) generating captions (BLIP) ...\")\n",
    "    captions_cache = os.path.join(workdir, \"captions.json\")\n",
    "    captions = generate_blip_captions(frames, cache_path=captions_cache) if not os.path.exists(captions_cache) else json.load(open(captions_cache))\n",
    "\n",
    "    # 5) compute caption/text embeddings (SentenceTransformer)\n",
    "    print(\"5) computing text embeddings for captions ...\")\n",
    "    txt_model_name = \"all-MiniLM-L6-v2\"  # small and fast; swap for larger if needed\n",
    "    txt_encoder = SentenceTransformer(txt_model_name, device=CONFIG[\"DEVICE\"])\n",
    "    caption_texts = [captions[os.path.basename(p)] for p in frames]\n",
    "    txt_embeddings = np.array(txt_encoder.encode(caption_texts, batch_size=CONFIG[\"BATCH_SIZE\"], show_progress_bar=True))\n",
    "\n",
    "    # 6) compute weights (multimodal uniqueness)\n",
    "    print(\"6) computing importance weights ...\")\n",
    "    # Option: combine image + text embeddings (concatenate) then compute uniqueness\n",
    "    img_embs_norm = img_embeddings / np.linalg.norm(img_embeddings, axis=1, keepdims=True)\n",
    "    txt_embs_norm = txt_embeddings / np.linalg.norm(txt_embeddings, axis=1, keepdims=True)\n",
    "    combined = np.concatenate([img_embs_norm, txt_embs_norm], axis=1)  # (T, 512+384)\n",
    "    sim = util.cos_sim(torch.tensor(combined), torch.tensor(combined)).cpu().numpy()\n",
    "    weights = 1 - sim.mean(axis=1)   # uniqueness score\n",
    "\n",
    "    # 7) cluster scenes and pick representative frames per cluster\n",
    "    print(\"7) clustering scenes and selecting keyframes ...\")\n",
    "    n_clusters = min(CONFIG[\"NUM_SCENES\"], len(frames))\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(combined)\n",
    "    selected_frames = []\n",
    "    for cidx in range(n_clusters):\n",
    "        members = np.where(kmeans.labels_ == cidx)[0]\n",
    "        if len(members) == 0:\n",
    "            continue\n",
    "        best = members[np.argmax(weights[members])]\n",
    "        selected_frames.append(frames[best])\n",
    "\n",
    "    # 8) create weighted caption list so summary emphasizes important frames\n",
    "    print(\"8) building weighted caption list ...\")\n",
    "    # create a mapping: frame basename -> weight\n",
    "    name_to_weight = {os.path.basename(frames[i]): float(weights[i]) for i in range(len(frames))}\n",
    "    weighted_caps = []\n",
    "    max_w = max(weights) if len(weights)>0 else 1.0\n",
    "    for f in frames:\n",
    "        cap = captions[os.path.basename(f)]\n",
    "        w = name_to_weight[os.path.basename(f)]\n",
    "        # scale repeats by weight (tune scale_factor as needed)\n",
    "        repeats = int(np.clip((w / (max_w + 1e-9)) * 4, 1, 6))\n",
    "        weighted_caps.extend([cap]*repeats)\n",
    "\n",
    "    # 9) generate text summary (T5)\n",
    "    print(\"9) generating summary with T5 ...\")\n",
    "    summary = generate_summary_from_texts(weighted_caps, model_name=CONFIG[\"SUMMARY_MODEL\"], max_len=120)\n",
    "\n",
    "    # 10) produce final outputs\n",
    "    outputs = {\n",
    "        \"frames\": frames,\n",
    "        \"selected_keyframes\": [os.path.basename(p) for p in selected_frames],\n",
    "        \"captions\": captions,\n",
    "        \"transcript\": transcript,\n",
    "        \"summary\": summary,\n",
    "        \"weights\": {os.path.basename(frames[i]): float(weights[i]) for i in range(len(frames))}\n",
    "    }\n",
    "    with open(os.path.join(workdir, \"clipinsight_output.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(outputs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\nâœ… Pipeline finished. Results written to:\", workdir)\n",
    "    print(\"Summary:\\n\", summary)\n",
    "    return outputs\n",
    "\n",
    "# ---------------------------\n",
    "# If run as script\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    out = pipeline_run(CONFIG[\"VIDEO_PATH\"], CONFIG[\"WORKDIR\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1abb52d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import tempfile\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119626ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
